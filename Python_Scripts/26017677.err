cpu-bind=MASK - host211, task  0  0 [25593]: mask 0xf0f set
cpu-bind=MASK - host211, task  0  0 [25922]: mask 0xf0f set
distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
distributed.scheduler - INFO - Clear task state
distributed.scheduler - INFO -   Scheduler at:  tcp://172.26.66.67:35806
distributed.scheduler - INFO -   dashboard at:                     :8787
distributed.diskutils - INFO - Found stale lock file and directory '/home/users/hkhatri/Git_Repo/AMOC_Variability_DePreSys_MET/Python_Scripts/dask-worker-space/worker-zr27u3_n', purging
distributed.scheduler - INFO - Receive client connection: Client-061dcee6-94b4-11ec-a544-f452141d4050
distributed.diskutils - INFO - Found stale lock file and directory '/home/users/hkhatri/Git_Repo/AMOC_Variability_DePreSys_MET/Python_Scripts/dask-worker-space/worker-4ev6b4m3', purging
distributed.core - INFO - Starting established connection
distributed.worker - INFO -       Start worker at:   tcp://172.26.66.67:37924
distributed.worker - INFO -       Start worker at:   tcp://172.26.66.67:40471
distributed.worker - INFO -          Listening to:   tcp://172.26.66.67:37924
distributed.worker - INFO -          Listening to:   tcp://172.26.66.67:40471
distributed.worker - INFO -          dashboard at:         172.26.66.67:45583
distributed.worker - INFO -          dashboard at:         172.26.66.67:39458
distributed.worker - INFO - Waiting to connect to:   tcp://172.26.66.67:35806
distributed.worker - INFO - Waiting to connect to:   tcp://172.26.66.67:35806
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                   3.91 GiB
distributed.worker - INFO -                Memory:                   3.91 GiB
distributed.worker - INFO -       Local Directory: /home/users/hkhatri/Git_Repo/AMOC_Variability_DePreSys_MET/Python_Scripts/dask-worker-space/worker-n141ewsr
distributed.worker - INFO -       Local Directory: /home/users/hkhatri/Git_Repo/AMOC_Variability_DePreSys_MET/Python_Scripts/dask-worker-space/worker-qacrpvhn
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:   tcp://172.26.66.67:37789
distributed.worker - INFO -          Listening to:   tcp://172.26.66.67:37789
distributed.worker - INFO -          dashboard at:         172.26.66.67:36349
distributed.worker - INFO -       Start worker at:   tcp://172.26.66.67:45799
distributed.worker - INFO - Waiting to connect to:   tcp://172.26.66.67:35806
distributed.worker - INFO -          Listening to:   tcp://172.26.66.67:45799
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -          dashboard at:         172.26.66.67:34014
distributed.worker - INFO -                Memory:                   3.91 GiB
distributed.worker - INFO - Waiting to connect to:   tcp://172.26.66.67:35806
distributed.worker - INFO -       Local Directory: /home/users/hkhatri/Git_Repo/AMOC_Variability_DePreSys_MET/Python_Scripts/dask-worker-space/worker-36iup4_8
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                   3.91 GiB
distributed.worker - INFO -       Local Directory: /home/users/hkhatri/Git_Repo/AMOC_Variability_DePreSys_MET/Python_Scripts/dask-worker-space/worker-fq25_krb
distributed.worker - INFO - -------------------------------------------------
distributed.diskutils - INFO - Found stale lock file and directory '/home/users/hkhatri/Git_Repo/AMOC_Variability_DePreSys_MET/Python_Scripts/dask-worker-space/worker-xtp6a596', purging
distributed.worker - INFO -       Start worker at:   tcp://172.26.66.67:43394
distributed.worker - INFO -          Listening to:   tcp://172.26.66.67:43394
distributed.worker - INFO -          dashboard at:         172.26.66.67:43530
distributed.worker - INFO - Waiting to connect to:   tcp://172.26.66.67:35806
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                   3.91 GiB
distributed.worker - INFO -       Local Directory: /home/users/hkhatri/Git_Repo/AMOC_Variability_DePreSys_MET/Python_Scripts/dask-worker-space/worker-fg5us25z
distributed.worker - INFO - -------------------------------------------------
distributed.scheduler - INFO - Register worker <WorkerState 'tcp://172.26.66.67:40471', name: 7, status: undefined, memory: 0, processing: 0>
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.26.66.67:40471
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:   tcp://172.26.66.67:35806
distributed.worker - INFO - -------------------------------------------------
distributed.scheduler - INFO - Register worker <WorkerState 'tcp://172.26.66.67:37924', name: 2, status: undefined, memory: 0, processing: 0>
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.26.66.67:37924
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:   tcp://172.26.66.67:35806
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Register worker <WorkerState 'tcp://172.26.66.67:37789', name: 3, status: undefined, memory: 0, processing: 0>
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.26.66.67:37789
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:   tcp://172.26.66.67:35806
distributed.scheduler - INFO - Register worker <WorkerState 'tcp://172.26.66.67:45799', name: 6, status: undefined, memory: 0, processing: 0>
distributed.worker - INFO - -------------------------------------------------
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.26.66.67:45799
distributed.core - INFO - Starting established connection
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:   tcp://172.26.66.67:35806
distributed.worker - INFO - -------------------------------------------------
distributed.scheduler - INFO - Register worker <WorkerState 'tcp://172.26.66.67:43394', name: 5, status: undefined, memory: 0, processing: 0>
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.26.66.67:43394
distributed.core - INFO - Starting established connection
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:   tcp://172.26.66.67:35806
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.diskutils - INFO - Found stale lock file and directory '/home/users/hkhatri/Git_Repo/AMOC_Variability_DePreSys_MET/Python_Scripts/dask-worker-space/worker-du9isz_9', purging
distributed.diskutils - INFO - Found stale lock file and directory '/home/users/hkhatri/Git_Repo/AMOC_Variability_DePreSys_MET/Python_Scripts/dask-worker-space/worker-mjxf3lgz', purging
distributed.diskutils - INFO - Found stale lock file and directory '/home/users/hkhatri/Git_Repo/AMOC_Variability_DePreSys_MET/Python_Scripts/dask-worker-space/worker-ntrdlvzh', purging
distributed.worker - INFO -       Start worker at:   tcp://172.26.66.67:35471
distributed.worker - INFO -          Listening to:   tcp://172.26.66.67:35471
distributed.worker - INFO -          dashboard at:         172.26.66.67:41130
distributed.worker - INFO - Waiting to connect to:   tcp://172.26.66.67:35806
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                   3.91 GiB
distributed.worker - INFO -       Local Directory: /home/users/hkhatri/Git_Repo/AMOC_Variability_DePreSys_MET/Python_Scripts/dask-worker-space/worker-gc0244a8
distributed.worker - INFO - -------------------------------------------------
distributed.scheduler - INFO - Register worker <WorkerState 'tcp://172.26.66.67:35471', name: 4, status: undefined, memory: 0, processing: 0>
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.26.66.67:35471
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:   tcp://172.26.66.67:35806
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO - Run out-of-band function 'lambda'
distributed.worker - INFO - Run out-of-band function 'lambda'
distributed.worker - INFO - Run out-of-band function 'lambda'
distributed.core - INFO - Event loop was unresponsive in Worker for 8.80s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 11.87s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 21.61s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
slurmstepd: error: Detected 286 oom-kill event(s) in step 26017677.0 cgroup. Some of your processes may have been killed by the cgroup out-of-memory handler.
srun: error: host211: task 0: Out Of Memory
[mpiexec@host211.jc.rl.ac.uk] HYDT_bscu_wait_for_completion (tools/bootstrap/utils/bscu_wait.c:73): one of the processes terminated badly; aborting
[mpiexec@host211.jc.rl.ac.uk] HYDT_bsci_wait_for_completion (tools/bootstrap/src/bsci_wait.c:21): launcher returned error waiting for completion
[mpiexec@host211.jc.rl.ac.uk] HYD_pmci_wait_for_completion (pm/pmiserv/pmiserv_pmci.c:179): launcher returned error waiting for completion
[mpiexec@host211.jc.rl.ac.uk] main (ui/mpich/mpiexec.c:326): process manager error waiting for completion
