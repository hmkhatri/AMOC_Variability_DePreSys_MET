2023-02-10 14:15:02,581 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
2023-02-10 14:15:02,612 - distributed.scheduler - INFO - State start
2023-02-10 14:15:02,616 - distributed.scheduler - INFO -   Scheduler at:  tcp://172.17.11.31:32963
2023-02-10 14:15:02,616 - distributed.scheduler - INFO -   dashboard at:                     :8787
2023-02-10 14:15:02,635 - distributed.worker - INFO -       Start worker at:   tcp://172.17.11.42:39913
2023-02-10 14:15:02,635 - distributed.worker - INFO -          Listening to:   tcp://172.17.11.42:39913
2023-02-10 14:15:02,635 - distributed.worker - INFO -           Worker name:                          6
2023-02-10 14:15:02,635 - distributed.worker - INFO -          dashboard at:         172.17.11.42:33891
2023-02-10 14:15:02,635 - distributed.worker - INFO - Waiting to connect to:   tcp://172.17.11.31:32963
2023-02-10 14:15:02,635 - distributed.worker - INFO - -------------------------------------------------
2023-02-10 14:15:02,635 - distributed.worker - INFO -               Threads:                          1
2023-02-10 14:15:02,635 - distributed.worker - INFO -                Memory:                   3.91 GiB
2023-02-10 14:15:02,635 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space-7053462/worker-9s0_v4zd
2023-02-10 14:15:02,635 - distributed.worker - INFO - -------------------------------------------------
2023-02-10 14:15:02,635 - distributed.worker - INFO -       Start worker at:   tcp://172.17.11.32:33861
2023-02-10 14:15:02,635 - distributed.worker - INFO -          Listening to:   tcp://172.17.11.32:33861
2023-02-10 14:15:02,635 - distributed.worker - INFO -           Worker name:                          3
2023-02-10 14:15:02,635 - distributed.worker - INFO -          dashboard at:         172.17.11.32:37222
2023-02-10 14:15:02,635 - distributed.worker - INFO - Waiting to connect to:   tcp://172.17.11.31:32963
2023-02-10 14:15:02,635 - distributed.worker - INFO - -------------------------------------------------
2023-02-10 14:15:02,635 - distributed.worker - INFO -               Threads:                          1
2023-02-10 14:15:02,635 - distributed.worker - INFO -                Memory:                   3.91 GiB
2023-02-10 14:15:02,635 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space-7053462/worker-af4j81nk
2023-02-10 14:15:02,635 - distributed.worker - INFO -       Start worker at:   tcp://172.17.11.42:39269
2023-02-10 14:15:02,635 - distributed.worker - INFO -          Listening to:   tcp://172.17.11.42:39269
2023-02-10 14:15:02,635 - distributed.worker - INFO -           Worker name:                          7
2023-02-10 14:15:02,635 - distributed.worker - INFO -          dashboard at:         172.17.11.42:40411
2023-02-10 14:15:02,635 - distributed.worker - INFO - Waiting to connect to:   tcp://172.17.11.31:32963
2023-02-10 14:15:02,635 - distributed.worker - INFO - -------------------------------------------------
2023-02-10 14:15:02,635 - distributed.worker - INFO -               Threads:                          1
2023-02-10 14:15:02,635 - distributed.worker - INFO -                Memory:                   3.91 GiB
2023-02-10 14:15:02,635 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space-7053462/worker-cr14uz0k
2023-02-10 14:15:02,636 - distributed.worker - INFO - -------------------------------------------------
2023-02-10 14:15:02,634 - distributed.worker - INFO -       Start worker at:   tcp://172.17.11.32:46765
2023-02-10 14:15:02,635 - distributed.worker - INFO -          Listening to:   tcp://172.17.11.32:46765
2023-02-10 14:15:02,635 - distributed.worker - INFO -           Worker name:                          4
2023-02-10 14:15:02,635 - distributed.worker - INFO -          dashboard at:         172.17.11.32:45291
2023-02-10 14:15:02,635 - distributed.worker - INFO - Waiting to connect to:   tcp://172.17.11.31:32963
2023-02-10 14:15:02,635 - distributed.worker - INFO - -------------------------------------------------
2023-02-10 14:15:02,635 - distributed.worker - INFO -               Threads:                          1
2023-02-10 14:15:02,635 - distributed.worker - INFO -                Memory:                   3.91 GiB
2023-02-10 14:15:02,635 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space-7053462/worker-llorrqfp
2023-02-10 14:15:02,635 - distributed.worker - INFO - -------------------------------------------------
2023-02-10 14:15:02,634 - distributed.worker - INFO -       Start worker at:   tcp://172.17.11.32:42429
2023-02-10 14:15:02,635 - distributed.worker - INFO -          Listening to:   tcp://172.17.11.32:42429
2023-02-10 14:15:02,635 - distributed.worker - INFO -           Worker name:                          5
2023-02-10 14:15:02,635 - distributed.worker - INFO -          dashboard at:         172.17.11.32:40371
2023-02-10 14:15:02,635 - distributed.worker - INFO - Waiting to connect to:   tcp://172.17.11.31:32963
2023-02-10 14:15:02,635 - distributed.worker - INFO - -------------------------------------------------
2023-02-10 14:15:02,635 - distributed.worker - INFO -               Threads:                          1
2023-02-10 14:15:02,635 - distributed.worker - INFO -                Memory:                   3.91 GiB
2023-02-10 14:15:02,635 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space-7053462/worker-q_70m2_x
2023-02-10 14:15:02,635 - distributed.worker - INFO - -------------------------------------------------
2023-02-10 14:15:02,635 - distributed.worker - INFO - -------------------------------------------------
2023-02-10 14:15:02,637 - distributed.scheduler - INFO - Receive client connection: Client-4f4a5743-a94d-11ed-a70a-1c34da4be456
2023-02-10 14:15:02,639 - distributed.worker - INFO -       Start worker at:   tcp://172.17.11.31:46096
2023-02-10 14:15:02,639 - distributed.worker - INFO -          Listening to:   tcp://172.17.11.31:46096
2023-02-10 14:15:02,639 - distributed.worker - INFO -           Worker name:                          2
2023-02-10 14:15:02,639 - distributed.worker - INFO -          dashboard at:         172.17.11.31:46871
2023-02-10 14:15:02,639 - distributed.worker - INFO - Waiting to connect to:   tcp://172.17.11.31:32963
2023-02-10 14:15:02,639 - distributed.worker - INFO - -------------------------------------------------
2023-02-10 14:15:02,640 - distributed.worker - INFO -               Threads:                          1
2023-02-10 14:15:02,640 - distributed.worker - INFO -                Memory:                   3.91 GiB
2023-02-10 14:15:02,640 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space-7053462/worker-k47ny9wy
2023-02-10 14:15:02,640 - distributed.worker - INFO - -------------------------------------------------
2023-02-10 14:15:02,653 - distributed.core - INFO - Starting established connection to tcp://172.17.11.31:51592
2023-02-10 14:15:02,655 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://172.17.11.32:46765', name: 4, status: init, memory: 0, processing: 0>
2023-02-10 14:15:02,656 - distributed.scheduler - INFO - Starting worker compute stream, tcp://172.17.11.32:46765
2023-02-10 14:15:02,656 - distributed.core - INFO - Starting established connection to tcp://172.17.11.32:45824
2023-02-10 14:15:02,657 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://172.17.11.32:42429', name: 5, status: init, memory: 0, processing: 0>
2023-02-10 14:15:02,656 - distributed.worker - INFO -         Registered to:   tcp://172.17.11.31:32963
2023-02-10 14:15:02,656 - distributed.worker - INFO - -------------------------------------------------
2023-02-10 14:15:02,657 - distributed.core - INFO - Starting established connection to tcp://172.17.11.31:32963
2023-02-10 14:15:02,657 - distributed.scheduler - INFO - Starting worker compute stream, tcp://172.17.11.32:42429
2023-02-10 14:15:02,657 - distributed.core - INFO - Starting established connection to tcp://172.17.11.32:45826
2023-02-10 14:15:02,657 - distributed.worker - INFO -         Registered to:   tcp://172.17.11.31:32963
2023-02-10 14:15:02,658 - distributed.worker - INFO - -------------------------------------------------
2023-02-10 14:15:02,658 - distributed.core - INFO - Starting established connection to tcp://172.17.11.31:32963
2023-02-10 14:15:02,666 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://172.17.11.42:39913', name: 6, status: init, memory: 0, processing: 0>
2023-02-10 14:15:02,666 - distributed.scheduler - INFO - Starting worker compute stream, tcp://172.17.11.42:39913
2023-02-10 14:15:02,667 - distributed.core - INFO - Starting established connection to tcp://172.17.11.42:49522
2023-02-10 14:15:02,667 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://172.17.11.42:39269', name: 7, status: init, memory: 0, processing: 0>
2023-02-10 14:15:02,667 - distributed.worker - INFO -         Registered to:   tcp://172.17.11.31:32963
2023-02-10 14:15:02,667 - distributed.worker - INFO - -------------------------------------------------
2023-02-10 14:15:02,667 - distributed.scheduler - INFO - Starting worker compute stream, tcp://172.17.11.42:39269
2023-02-10 14:15:02,667 - distributed.core - INFO - Starting established connection to tcp://172.17.11.42:49524
2023-02-10 14:15:02,667 - distributed.core - INFO - Starting established connection to tcp://172.17.11.31:32963
2023-02-10 14:15:02,667 - distributed.worker - INFO -         Registered to:   tcp://172.17.11.31:32963
2023-02-10 14:15:02,668 - distributed.worker - INFO - -------------------------------------------------
2023-02-10 14:15:02,668 - distributed.core - INFO - Starting established connection to tcp://172.17.11.31:32963
2023-02-10 14:15:02,670 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://172.17.11.31:46096', name: 2, status: init, memory: 0, processing: 0>
2023-02-10 14:15:02,670 - distributed.scheduler - INFO - Starting worker compute stream, tcp://172.17.11.31:46096
2023-02-10 14:15:02,670 - distributed.core - INFO - Starting established connection to tcp://172.17.11.31:51596
2023-02-10 14:15:02,671 - distributed.worker - INFO -         Registered to:   tcp://172.17.11.31:32963
2023-02-10 14:15:02,671 - distributed.worker - INFO - -------------------------------------------------
2023-02-10 14:15:02,671 - distributed.core - INFO - Starting established connection to tcp://172.17.11.31:32963
2023-02-10 14:15:02,673 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://172.17.11.32:33861', name: 3, status: init, memory: 0, processing: 0>
2023-02-10 14:15:02,673 - distributed.scheduler - INFO - Starting worker compute stream, tcp://172.17.11.32:33861
2023-02-10 14:15:02,673 - distributed.core - INFO - Starting established connection to tcp://172.17.11.32:45828
2023-02-10 14:15:02,673 - distributed.worker - INFO -         Registered to:   tcp://172.17.11.31:32963
2023-02-10 14:15:02,674 - distributed.worker - INFO - -------------------------------------------------
2023-02-10 14:15:02,674 - distributed.core - INFO - Starting established connection to tcp://172.17.11.31:32963
2023-02-10 14:46:00,760 - distributed.utils_perf - INFO - full garbage collection released 39.33 MiB from 790 reference cycles (threshold: 9.54 MiB)
2023-02-10 15:27:45,625 - distributed.utils_perf - INFO - full garbage collection released 52.26 MiB from 710 reference cycles (threshold: 9.54 MiB)
2023-02-10 15:36:38,013 - distributed.core - INFO - Event loop was unresponsive in Scheduler for 3.08s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-02-10 16:31:15,633 - distributed.utils_perf - INFO - full garbage collection released 74.82 MiB from 729 reference cycles (threshold: 9.54 MiB)
2023-02-10 16:38:54,837 - distributed.utils_perf - WARNING - full garbage collections took 10% CPU time recently (threshold: 10%)
2023-02-10 16:39:04,066 - distributed.utils_perf - WARNING - full garbage collections took 10% CPU time recently (threshold: 10%)
2023-02-10 16:45:58,257 - distributed.utils_perf - WARNING - full garbage collections took 10% CPU time recently (threshold: 10%)
2023-02-10 16:49:56,402 - distributed.utils_perf - WARNING - full garbage collections took 10% CPU time recently (threshold: 10%)
2023-02-10 16:50:07,157 - distributed.utils_perf - WARNING - full garbage collections took 10% CPU time recently (threshold: 10%)
2023-02-10 16:57:02,139 - distributed.utils_perf - WARNING - full garbage collections took 10% CPU time recently (threshold: 10%)
2023-02-10 16:57:04,105 - distributed.utils_perf - WARNING - full garbage collections took 11% CPU time recently (threshold: 10%)
2023-02-10 17:06:15,409 - distributed.utils_perf - WARNING - full garbage collections took 10% CPU time recently (threshold: 10%)
2023-02-10 17:06:15,763 - distributed.utils_perf - WARNING - full garbage collections took 10% CPU time recently (threshold: 10%)
2023-02-10 17:06:16,298 - distributed.utils_perf - WARNING - full garbage collections took 10% CPU time recently (threshold: 10%)
2023-02-10 17:06:16,814 - distributed.utils_perf - WARNING - full garbage collections took 11% CPU time recently (threshold: 10%)
2023-02-10 17:06:23,417 - distributed.utils_perf - INFO - full garbage collection released 226.47 MiB from 856 reference cycles (threshold: 9.54 MiB)
2023-02-10 17:06:29,072 - distributed.utils_perf - WARNING - full garbage collections took 10% CPU time recently (threshold: 10%)
2023-02-10 17:13:59,872 - distributed.utils_perf - WARNING - full garbage collections took 10% CPU time recently (threshold: 10%)
2023-02-10 17:14:00,301 - distributed.utils_perf - WARNING - full garbage collections took 10% CPU time recently (threshold: 10%)
2023-02-10 17:14:00,881 - distributed.utils_perf - WARNING - full garbage collections took 10% CPU time recently (threshold: 10%)
2023-02-10 17:14:01,785 - distributed.utils_perf - WARNING - full garbage collections took 11% CPU time recently (threshold: 10%)
2023-02-10 17:18:40,053 - distributed.utils_perf - WARNING - full garbage collections took 11% CPU time recently (threshold: 10%)
2023-02-10 17:18:40,466 - distributed.core - INFO - Event loop was unresponsive in Scheduler for 3.02s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-02-10 17:18:52,191 - distributed.utils_perf - WARNING - full garbage collections took 10% CPU time recently (threshold: 10%)
2023-02-10 17:23:59,459 - distributed.utils_perf - WARNING - full garbage collections took 10% CPU time recently (threshold: 10%)
2023-02-10 17:24:00,204 - distributed.utils_perf - WARNING - full garbage collections took 10% CPU time recently (threshold: 10%)
2023-02-10 17:24:01,186 - distributed.utils_perf - WARNING - full garbage collections took 10% CPU time recently (threshold: 10%)
2023-02-10 17:24:03,134 - distributed.utils_perf - WARNING - full garbage collections took 11% CPU time recently (threshold: 10%)
2023-02-10 17:31:12,734 - distributed.utils_perf - WARNING - full garbage collections took 10% CPU time recently (threshold: 10%)
2023-02-10 17:31:13,139 - distributed.utils_perf - WARNING - full garbage collections took 10% CPU time recently (threshold: 10%)
2023-02-10 17:31:13,711 - distributed.utils_perf - WARNING - full garbage collections took 10% CPU time recently (threshold: 10%)
2023-02-10 17:31:14,228 - distributed.utils_perf - WARNING - full garbage collections took 11% CPU time recently (threshold: 10%)
2023-02-10 17:31:14,599 - distributed.core - INFO - Event loop was unresponsive in Scheduler for 3.11s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-02-10 17:31:30,617 - distributed.utils_perf - WARNING - full garbage collections took 10% CPU time recently (threshold: 10%)
2023-02-10 17:36:59,487 - distributed.utils_perf - WARNING - full garbage collections took 10% CPU time recently (threshold: 10%)
2023-02-10 17:36:59,928 - distributed.utils_perf - WARNING - full garbage collections took 10% CPU time recently (threshold: 10%)
2023-02-10 17:37:00,557 - distributed.utils_perf - WARNING - full garbage collections took 10% CPU time recently (threshold: 10%)
2023-02-10 17:37:07,909 - distributed.utils_perf - WARNING - full garbage collections took 11% CPU time recently (threshold: 10%)
2023-02-10 17:45:13,848 - distributed.utils_perf - WARNING - full garbage collections took 10% CPU time recently (threshold: 10%)
2023-02-10 17:45:14,330 - distributed.utils_perf - WARNING - full garbage collections took 10% CPU time recently (threshold: 10%)
2023-02-10 17:45:16,018 - distributed.utils_perf - WARNING - full garbage collections took 11% CPU time recently (threshold: 10%)
2023-02-10 17:55:20,636 - distributed.utils_perf - WARNING - full garbage collections took 10% CPU time recently (threshold: 10%)
2023-02-10 17:55:30,941 - distributed.utils_perf - WARNING - full garbage collections took 10% CPU time recently (threshold: 10%)
2023-02-10 18:02:25,028 - distributed.utils_perf - WARNING - full garbage collections took 10% CPU time recently (threshold: 10%)
2023-02-10 18:27:58,214 - distributed.core - INFO - Event loop was unresponsive in Scheduler for 4.07s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-02-10 18:36:47,955 - distributed.core - INFO - Event loop was unresponsive in Scheduler for 3.61s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-02-10 20:41:56,571 - distributed.core - INFO - Event loop was unresponsive in Scheduler for 3.03s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-02-10 20:49:11,861 - distributed.core - INFO - Event loop was unresponsive in Scheduler for 3.09s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-02-10 20:55:45,464 - distributed.utils_perf - WARNING - full garbage collections took 10% CPU time recently (threshold: 10%)
2023-02-10 20:55:45,999 - distributed.utils_perf - WARNING - full garbage collections took 10% CPU time recently (threshold: 10%)
2023-02-10 20:55:46,680 - distributed.utils_perf - WARNING - full garbage collections took 10% CPU time recently (threshold: 10%)
2023-02-10 20:55:48,051 - distributed.utils_perf - WARNING - full garbage collections took 10% CPU time recently (threshold: 10%)
2023-02-10 21:05:34,330 - distributed.utils_perf - WARNING - full garbage collections took 10% CPU time recently (threshold: 10%)
2023-02-10 21:05:34,770 - distributed.utils_perf - WARNING - full garbage collections took 10% CPU time recently (threshold: 10%)
2023-02-10 21:05:51,471 - distributed.utils_perf - WARNING - full garbage collections took 10% CPU time recently (threshold: 10%)
2023-02-10 21:58:44,570 - distributed.core - INFO - Event loop was unresponsive in Scheduler for 3.08s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-02-10 21:58:48,533 - distributed.core - INFO - Event loop was unresponsive in Scheduler for 3.96s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-02-10 22:07:01,201 - distributed.core - INFO - Event loop was unresponsive in Scheduler for 3.75s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-02-10 22:28:54,222 - distributed.core - INFO - Event loop was unresponsive in Scheduler for 3.14s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-02-10 23:12:03,671 - distributed.core - INFO - Event loop was unresponsive in Scheduler for 3.88s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-02-10 23:17:08,627 - distributed.core - INFO - Event loop was unresponsive in Scheduler for 3.02s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-02-10 23:29:13,478 - distributed.utils_perf - WARNING - full garbage collections took 10% CPU time recently (threshold: 10%)
2023-02-10 23:29:14,182 - distributed.utils_perf - WARNING - full garbage collections took 10% CPU time recently (threshold: 10%)
2023-02-10 23:29:17,504 - distributed.utils_perf - WARNING - full garbage collections took 10% CPU time recently (threshold: 10%)
2023-02-10 23:36:03,311 - distributed.utils_perf - WARNING - full garbage collections took 10% CPU time recently (threshold: 10%)
2023-02-10 23:36:03,928 - distributed.utils_perf - WARNING - full garbage collections took 10% CPU time recently (threshold: 10%)
2023-02-10 23:36:04,768 - distributed.utils_perf - WARNING - full garbage collections took 10% CPU time recently (threshold: 10%)
2023-02-10 23:42:57,075 - distributed.utils_perf - WARNING - full garbage collections took 10% CPU time recently (threshold: 10%)
2023-02-10 23:42:57,869 - distributed.utils_perf - WARNING - full garbage collections took 10% CPU time recently (threshold: 10%)
2023-02-10 23:42:58,611 - distributed.utils_perf - WARNING - full garbage collections took 10% CPU time recently (threshold: 10%)
2023-02-10 23:42:59,005 - distributed.core - INFO - Event loop was unresponsive in Scheduler for 3.10s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-02-10 23:50:13,687 - distributed.utils_perf - WARNING - full garbage collections took 10% CPU time recently (threshold: 10%)
2023-02-10 23:50:14,438 - distributed.utils_perf - WARNING - full garbage collections took 10% CPU time recently (threshold: 10%)
2023-02-10 23:50:15,255 - distributed.utils_perf - WARNING - full garbage collections took 10% CPU time recently (threshold: 10%)
2023-02-10 23:50:33,000 - distributed.utils_perf - WARNING - full garbage collections took 10% CPU time recently (threshold: 10%)
2023-02-10 23:55:49,819 - distributed.utils_perf - WARNING - full garbage collections took 10% CPU time recently (threshold: 10%)
2023-02-10 23:55:50,401 - distributed.utils_perf - WARNING - full garbage collections took 10% CPU time recently (threshold: 10%)
2023-02-10 23:55:51,204 - distributed.utils_perf - WARNING - full garbage collections took 10% CPU time recently (threshold: 10%)
2023-02-10 23:56:02,071 - distributed.utils_perf - WARNING - full garbage collections took 10% CPU time recently (threshold: 10%)
2023-02-11 00:00:35,208 - distributed.utils_perf - WARNING - full garbage collections took 10% CPU time recently (threshold: 10%)
2023-02-11 00:07:24,854 - distributed.utils_perf - WARNING - full garbage collections took 10% CPU time recently (threshold: 10%)
2023-02-11 00:07:25,904 - distributed.utils_perf - WARNING - full garbage collections took 10% CPU time recently (threshold: 10%)
2023-02-11 00:07:26,480 - distributed.core - INFO - Event loop was unresponsive in Scheduler for 3.10s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-02-11 00:07:30,982 - distributed.utils_perf - WARNING - full garbage collections took 10% CPU time recently (threshold: 10%)
2023-02-11 00:15:36,934 - distributed.utils_perf - WARNING - full garbage collections took 10% CPU time recently (threshold: 10%)
2023-02-11 00:23:56,924 - distributed.utils_perf - WARNING - full garbage collections took 10% CPU time recently (threshold: 10%)
2023-02-11 00:33:20,104 - distributed.utils_perf - WARNING - full garbage collections took 10% CPU time recently (threshold: 10%)
2023-02-11 00:33:20,768 - distributed.utils_perf - WARNING - full garbage collections took 10% CPU time recently (threshold: 10%)
2023-02-11 00:33:21,920 - distributed.utils_perf - WARNING - full garbage collections took 10% CPU time recently (threshold: 10%)
2023-02-11 01:38:35,607 - distributed.core - INFO - Event loop was unresponsive in Scheduler for 3.04s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-02-11 01:46:40,342 - distributed.core - INFO - Event loop was unresponsive in Scheduler for 3.34s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-02-11 02:36:59,947 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.64s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-02-11 02:37:00,561 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.09s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-02-11 02:37:00,568 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.20s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-02-11 03:16:58,697 - distributed.core - INFO - Event loop was unresponsive in Scheduler for 4.04s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-02-11 04:13:04,770 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.22s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-02-11 04:33:17,114 - distributed.core - INFO - Event loop was unresponsive in Scheduler for 4.32s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-02-11 04:55:14,583 - distributed.utils_perf - WARNING - full garbage collections took 10% CPU time recently (threshold: 10%)
2023-02-11 05:04:33,141 - distributed.core - INFO - Event loop was unresponsive in Scheduler for 6.03s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-02-11 05:11:37,791 - distributed.core - INFO - Event loop was unresponsive in Scheduler for 3.48s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-02-11 05:30:24,657 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.41s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-02-11 05:30:24,671 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.07s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-02-11 05:44:22,445 - distributed.core - INFO - Event loop was unresponsive in Scheduler for 3.22s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-02-11 05:50:51,916 - distributed.core - INFO - Event loop was unresponsive in Scheduler for 4.79s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-02-11 07:27:41,798 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.12s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-02-11 08:44:52,879 - distributed.core - INFO - Event loop was unresponsive in Scheduler for 4.10s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-02-11 08:52:20,854 - distributed.core - INFO - Event loop was unresponsive in Scheduler for 3.41s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-02-11 10:18:00,843 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.16s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-02-11 10:18:00,844 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.30s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-02-11 10:35:42,169 - distributed.core - INFO - Event loop was unresponsive in Scheduler for 3.02s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-02-11 10:55:14,174 - distributed.core - INFO - Event loop was unresponsive in Scheduler for 4.18s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-02-11 12:30:32,981 - distributed.core - INFO - Event loop was unresponsive in Scheduler for 3.24s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-02-11 12:38:00,139 - distributed.core - INFO - Event loop was unresponsive in Scheduler for 4.78s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-02-11 12:38:20,488 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.36s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-02-11 12:38:20,518 - distributed.core - INFO - Event loop was unresponsive in Scheduler for 3.39s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-02-11 12:55:27,614 - distributed.core - INFO - Event loop was unresponsive in Scheduler for 3.33s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-02-11 13:10:35,346 - distributed.core - INFO - Event loop was unresponsive in Scheduler for 4.01s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-02-11 13:56:34,966 - distributed.core - INFO - Event loop was unresponsive in Scheduler for 3.62s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2023-02-11 14:11:10,637 - distributed.scheduler - INFO - Receive client connection: Client-ef6cf0ea-aa15-11ed-a70a-1c34da4be456
2023-02-11 14:11:10,638 - distributed.core - INFO - Starting established connection to tcp://172.17.11.31:49956
2023-02-11 14:11:10,640 - distributed.worker - INFO - Run out-of-band function 'stop'
2023-02-11 14:11:10,642 - distributed.worker - INFO - Stopping worker at tcp://172.17.11.42:39913. Reason: scheduler-close
2023-02-11 14:11:10,642 - distributed.worker - INFO - Stopping worker at tcp://172.17.11.42:39269. Reason: scheduler-close
2023-02-11 14:11:10,643 - distributed.core - INFO - Received 'close-stream' from tcp://172.17.11.31:32963; closing.
2023-02-11 14:11:10,643 - distributed.core - INFO - Received 'close-stream' from tcp://172.17.11.31:32963; closing.
2023-02-11 14:11:10,641 - distributed.scheduler - INFO - Scheduler closing...
2023-02-11 14:11:10,641 - distributed.scheduler - INFO - Scheduler closing all comms
2023-02-11 14:11:10,643 - distributed.worker - INFO - Stopping worker at tcp://172.17.11.31:46096. Reason: scheduler-close
2023-02-11 14:11:10,643 - distributed.worker - INFO - Stopping worker at tcp://172.17.11.32:46765. Reason: scheduler-close
2023-02-11 14:11:10,643 - distributed.worker - INFO - Stopping worker at tcp://172.17.11.32:42429. Reason: scheduler-close
2023-02-11 14:11:10,644 - distributed.core - INFO - Received 'close-stream' from tcp://172.17.11.31:32963; closing.
2023-02-11 14:11:10,644 - distributed.core - INFO - Received 'close-stream' from tcp://172.17.11.31:32963; closing.
2023-02-11 14:11:10,645 - distributed.worker - INFO - Stopping worker at tcp://172.17.11.32:33861. Reason: scheduler-close
2023-02-11 14:11:10,644 - distributed.core - INFO - Connection to tcp://172.17.11.42:49524 has been closed.
2023-02-11 14:11:10,645 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://172.17.11.42:39269', name: 7, status: running, memory: 0, processing: 0>
2023-02-11 14:11:10,645 - distributed.core - INFO - Removing comms to tcp://172.17.11.42:39269
2023-02-11 14:11:10,643 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://172.17.11.42:49524 remote=tcp://172.17.11.31:32963>
Traceback (most recent call last):
  File "/home/users/hkhatri/miniconda3/lib/python3.9/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/home/users/hkhatri/miniconda3/lib/python3.9/site-packages/tornado/gen.py", line 769, in run
    value = future.result()
  File "/home/users/hkhatri/miniconda3/lib/python3.9/site-packages/distributed/comm/tcp.py", line 269, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2023-02-11 14:11:10,645 - distributed.core - INFO - Received 'close-stream' from tcp://172.17.11.31:32963; closing.
2023-02-11 14:11:10,643 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://172.17.11.42:49522 remote=tcp://172.17.11.31:32963>
Traceback (most recent call last):
  File "/home/users/hkhatri/miniconda3/lib/python3.9/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/home/users/hkhatri/miniconda3/lib/python3.9/site-packages/tornado/gen.py", line 769, in run
    value = future.result()
  File "/home/users/hkhatri/miniconda3/lib/python3.9/site-packages/distributed/comm/tcp.py", line 269, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2023-02-11 14:11:10,645 - distributed.core - INFO - Connection to tcp://172.17.11.42:49522 has been closed.
2023-02-11 14:11:10,645 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://172.17.11.42:39913', name: 6, status: running, memory: 0, processing: 0>
2023-02-11 14:11:10,645 - distributed.core - INFO - Removing comms to tcp://172.17.11.42:39913
2023-02-11 14:11:10,645 - distributed.core - INFO - Connection to tcp://172.17.11.32:45824 has been closed.
2023-02-11 14:11:10,645 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://172.17.11.32:46765', name: 4, status: running, memory: 0, processing: 0>
2023-02-11 14:11:10,645 - distributed.core - INFO - Removing comms to tcp://172.17.11.32:46765
2023-02-11 14:11:10,645 - distributed.core - INFO - Connection to tcp://172.17.11.32:45826 has been closed.
2023-02-11 14:11:10,646 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://172.17.11.32:42429', name: 5, status: running, memory: 0, processing: 0>
2023-02-11 14:11:10,646 - distributed.core - INFO - Removing comms to tcp://172.17.11.32:42429
2023-02-11 14:11:10,646 - distributed.core - INFO - Connection to tcp://172.17.11.31:51596 has been closed.
2023-02-11 14:11:10,646 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://172.17.11.31:46096', name: 2, status: running, memory: 0, processing: 0>
2023-02-11 14:11:10,646 - distributed.core - INFO - Removing comms to tcp://172.17.11.31:46096
2023-02-11 14:11:10,646 - distributed.core - INFO - Connection to tcp://172.17.11.32:45828 has been closed.
2023-02-11 14:11:10,646 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://172.17.11.32:33861', name: 3, status: running, memory: 0, processing: 0>
2023-02-11 14:11:10,646 - distributed.core - INFO - Removing comms to tcp://172.17.11.32:33861
2023-02-11 14:11:10,646 - distributed.scheduler - INFO - Lost all workers
2023-02-11 14:11:10,645 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://172.17.11.32:45824 remote=tcp://172.17.11.31:32963>
Traceback (most recent call last):
  File "/home/users/hkhatri/miniconda3/lib/python3.9/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/home/users/hkhatri/miniconda3/lib/python3.9/site-packages/tornado/gen.py", line 769, in run
    value = future.result()
  File "/home/users/hkhatri/miniconda3/lib/python3.9/site-packages/distributed/comm/tcp.py", line 269, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2023-02-11 14:11:10,645 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://172.17.11.32:45826 remote=tcp://172.17.11.31:32963>
Traceback (most recent call last):
  File "/home/users/hkhatri/miniconda3/lib/python3.9/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/home/users/hkhatri/miniconda3/lib/python3.9/site-packages/tornado/gen.py", line 769, in run
    value = future.result()
  File "/home/users/hkhatri/miniconda3/lib/python3.9/site-packages/distributed/comm/tcp.py", line 269, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2023-02-11 14:11:10,646 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://172.17.11.32:45828 remote=tcp://172.17.11.31:32963>
Traceback (most recent call last):
  File "/home/users/hkhatri/miniconda3/lib/python3.9/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/home/users/hkhatri/miniconda3/lib/python3.9/site-packages/tornado/gen.py", line 769, in run
    value = future.result()
  File "/home/users/hkhatri/miniconda3/lib/python3.9/site-packages/distributed/comm/tcp.py", line 269, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2023-02-11 14:11:10,651 - distributed.core - INFO - Received 'close-stream' from tcp://172.17.11.31:32963; closing.
2023-02-11 14:11:10,652 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://172.17.11.31:51596 remote=tcp://172.17.11.31:32963>
Traceback (most recent call last):
  File "/home/users/hkhatri/miniconda3/lib/python3.9/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/home/users/hkhatri/miniconda3/lib/python3.9/site-packages/tornado/gen.py", line 769, in run
    value = future.result()
  File "/home/users/hkhatri/miniconda3/lib/python3.9/site-packages/distributed/comm/tcp.py", line 269, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2023-02-11 14:11:10,761 - distributed.client - ERROR - 
ConnectionRefusedError: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/users/hkhatri/miniconda3/lib/python3.9/site-packages/distributed/comm/core.py", line 291, in connect
    comm = await asyncio.wait_for(
  File "/home/users/hkhatri/miniconda3/lib/python3.9/asyncio/tasks.py", line 479, in wait_for
    return fut.result()
  File "/home/users/hkhatri/miniconda3/lib/python3.9/site-packages/distributed/comm/tcp.py", line 511, in connect
    convert_stream_closed_error(self, e)
  File "/home/users/hkhatri/miniconda3/lib/python3.9/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <distributed.comm.tcp.TCPConnector object at 0x7ffaeb5cae20>: ConnectionRefusedError: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/users/hkhatri/miniconda3/lib/python3.9/site-packages/distributed/utils.py", line 742, in wrapper
    return await func(*args, **kwargs)
  File "/home/users/hkhatri/miniconda3/lib/python3.9/site-packages/distributed/client.py", line 1301, in _reconnect
    await self._ensure_connected(timeout=timeout)
  File "/home/users/hkhatri/miniconda3/lib/python3.9/site-packages/distributed/client.py", line 1331, in _ensure_connected
    comm = await connect(
  File "/home/users/hkhatri/miniconda3/lib/python3.9/site-packages/distributed/comm/core.py", line 315, in connect
    await asyncio.sleep(backoff)
  File "/home/users/hkhatri/miniconda3/lib/python3.9/asyncio/tasks.py", line 652, in sleep
    return await future
asyncio.exceptions.CancelledError

Traceback (most recent call last):
  File "/home/users/hkhatri/miniconda3/lib/python3.9/site-packages/distributed/comm/tcp.py", line 225, in read
    frames_nbytes = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/users/hkhatri/miniconda3/lib/python3.9/site-packages/distributed/client.py", line 1503, in _handle_report
    msgs = await self.scheduler_comm.comm.read()
  File "/home/users/hkhatri/miniconda3/lib/python3.9/site-packages/distributed/comm/tcp.py", line 241, in read
    convert_stream_closed_error(self, e)
  File "/home/users/hkhatri/miniconda3/lib/python3.9/site-packages/distributed/comm/tcp.py", line 144, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Client->Scheduler local=tcp://172.17.11.31:49956 remote=tcp://172.17.11.31:32963>: Stream is closed

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/users/hkhatri/miniconda3/lib/python3.9/site-packages/distributed/utils.py", line 742, in wrapper
    return await func(*args, **kwargs)
  File "/home/users/hkhatri/miniconda3/lib/python3.9/site-packages/distributed/client.py", line 1511, in _handle_report
    await self._reconnect()
  File "/home/users/hkhatri/miniconda3/lib/python3.9/site-packages/distributed/utils.py", line 742, in wrapper
    return await func(*args, **kwargs)
  File "/home/users/hkhatri/miniconda3/lib/python3.9/site-packages/distributed/client.py", line 1301, in _reconnect
    await self._ensure_connected(timeout=timeout)
  File "/home/users/hkhatri/miniconda3/lib/python3.9/site-packages/distributed/client.py", line 1331, in _ensure_connected
    comm = await connect(
  File "/home/users/hkhatri/miniconda3/lib/python3.9/site-packages/distributed/comm/core.py", line 315, in connect
    await asyncio.sleep(backoff)
  File "/home/users/hkhatri/miniconda3/lib/python3.9/asyncio/tasks.py", line 652, in sleep
    return await future
asyncio.exceptions.CancelledError
2023-02-11 14:11:10,885 - distributed.client - ERROR - 
ConnectionRefusedError: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/users/hkhatri/miniconda3/lib/python3.9/site-packages/distributed/comm/core.py", line 291, in connect
    comm = await asyncio.wait_for(
  File "/home/users/hkhatri/miniconda3/lib/python3.9/asyncio/tasks.py", line 479, in wait_for
    return fut.result()
  File "/home/users/hkhatri/miniconda3/lib/python3.9/site-packages/distributed/comm/tcp.py", line 511, in connect
    convert_stream_closed_error(self, e)
  File "/home/users/hkhatri/miniconda3/lib/python3.9/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <distributed.comm.tcp.TCPConnector object at 0x7ff645a21250>: ConnectionRefusedError: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/users/hkhatri/miniconda3/lib/python3.9/site-packages/distributed/utils.py", line 742, in wrapper
    return await func(*args, **kwargs)
  File "/home/users/hkhatri/miniconda3/lib/python3.9/site-packages/distributed/client.py", line 1301, in _reconnect
    await self._ensure_connected(timeout=timeout)
  File "/home/users/hkhatri/miniconda3/lib/python3.9/site-packages/distributed/client.py", line 1331, in _ensure_connected
    comm = await connect(
  File "/home/users/hkhatri/miniconda3/lib/python3.9/site-packages/distributed/comm/core.py", line 315, in connect
    await asyncio.sleep(backoff)
  File "/home/users/hkhatri/miniconda3/lib/python3.9/asyncio/tasks.py", line 652, in sleep
    return await future
asyncio.exceptions.CancelledError

Traceback (most recent call last):
  File "/home/users/hkhatri/miniconda3/lib/python3.9/site-packages/distributed/comm/tcp.py", line 225, in read
    frames_nbytes = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/users/hkhatri/miniconda3/lib/python3.9/site-packages/distributed/client.py", line 1503, in _handle_report
    msgs = await self.scheduler_comm.comm.read()
  File "/home/users/hkhatri/miniconda3/lib/python3.9/site-packages/distributed/comm/tcp.py", line 241, in read
    convert_stream_closed_error(self, e)
  File "/home/users/hkhatri/miniconda3/lib/python3.9/site-packages/distributed/comm/tcp.py", line 144, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Client->Scheduler local=tcp://172.17.11.31:51592 remote=tcp://172.17.11.31:32963>: Stream is closed

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/users/hkhatri/miniconda3/lib/python3.9/site-packages/distributed/utils.py", line 742, in wrapper
    return await func(*args, **kwargs)
  File "/home/users/hkhatri/miniconda3/lib/python3.9/site-packages/distributed/client.py", line 1511, in _handle_report
    await self._reconnect()
  File "/home/users/hkhatri/miniconda3/lib/python3.9/site-packages/distributed/utils.py", line 742, in wrapper
    return await func(*args, **kwargs)
  File "/home/users/hkhatri/miniconda3/lib/python3.9/site-packages/distributed/client.py", line 1301, in _reconnect
    await self._ensure_connected(timeout=timeout)
  File "/home/users/hkhatri/miniconda3/lib/python3.9/site-packages/distributed/client.py", line 1331, in _ensure_connected
    comm = await connect(
  File "/home/users/hkhatri/miniconda3/lib/python3.9/site-packages/distributed/comm/core.py", line 315, in connect
    await asyncio.sleep(backoff)
  File "/home/users/hkhatri/miniconda3/lib/python3.9/asyncio/tasks.py", line 652, in sleep
    return await future
asyncio.exceptions.CancelledError
