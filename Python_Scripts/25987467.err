cpu-bind=MASK - host210, task  0  0 [9201]: mask 0xf0f set
cpu-bind=MASK - host210, task  0  0 [9932]: mask 0xf0f set
distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
distributed.scheduler - INFO - Clear task state
distributed.scheduler - INFO -   Scheduler at:  tcp://172.26.66.66:37206
distributed.scheduler - INFO -   dashboard at:                     :8787
distributed.scheduler - INFO - Receive client connection: Client-a9e78385-9491-11ec-a6ce-f452141d4040
distributed.core - INFO - Starting established connection
distributed.diskutils - INFO - Found stale lock file and directory '/home/users/hkhatri/Git_Repo/AMOC_Variability_DePreSys_MET/Python_Scripts/dask-worker-space/worker-l637tvqj', purging
distributed.diskutils - INFO - Found stale lock file and directory '/home/users/hkhatri/Git_Repo/AMOC_Variability_DePreSys_MET/Python_Scripts/dask-worker-space/worker-rk6wbgf1', purging
distributed.diskutils - INFO - Found stale lock file and directory '/home/users/hkhatri/Git_Repo/AMOC_Variability_DePreSys_MET/Python_Scripts/dask-worker-space/worker-17rlgonm', purging
distributed.worker - INFO -       Start worker at:   tcp://172.26.66.66:36666
distributed.worker - INFO -          Listening to:   tcp://172.26.66.66:36666
distributed.worker - INFO -          dashboard at:         172.26.66.66:44824
distributed.worker - INFO - Waiting to connect to:   tcp://172.26.66.66:37206
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                   3.91 GiB
distributed.worker - INFO -       Local Directory: /home/users/hkhatri/Git_Repo/AMOC_Variability_DePreSys_MET/Python_Scripts/dask-worker-space/worker-ys3sas34
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:   tcp://172.26.66.66:45002
distributed.worker - INFO -          Listening to:   tcp://172.26.66.66:45002
distributed.worker - INFO -          dashboard at:         172.26.66.66:38242
distributed.worker - INFO - Waiting to connect to:   tcp://172.26.66.66:37206
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                   3.91 GiB
distributed.worker - INFO -       Local Directory: /home/users/hkhatri/Git_Repo/AMOC_Variability_DePreSys_MET/Python_Scripts/dask-worker-space/worker-oczw63li
distributed.worker - INFO - -------------------------------------------------
distributed.diskutils - INFO - Found stale lock file and directory '/home/users/hkhatri/Git_Repo/AMOC_Variability_DePreSys_MET/Python_Scripts/dask-worker-space/worker-omwrdusz', purging
distributed.scheduler - INFO - Register worker <WorkerState 'tcp://172.26.66.66:45002', name: 5, status: undefined, memory: 0, processing: 0>
distributed.worker - INFO -       Start worker at:   tcp://172.26.66.66:34363
distributed.worker - INFO -          Listening to:   tcp://172.26.66.66:34363
distributed.worker - INFO -          dashboard at:         172.26.66.66:38709
distributed.worker - INFO - Waiting to connect to:   tcp://172.26.66.66:37206
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                   3.91 GiB
distributed.worker - INFO -       Local Directory: /home/users/hkhatri/Git_Repo/AMOC_Variability_DePreSys_MET/Python_Scripts/dask-worker-space/worker-5emrpx8e
distributed.worker - INFO - -------------------------------------------------
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.26.66.66:45002
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:   tcp://172.26.66.66:37206
distributed.worker - INFO - -------------------------------------------------
distributed.scheduler - INFO - Register worker <WorkerState 'tcp://172.26.66.66:36666', name: 3, status: undefined, memory: 0, processing: 0>
distributed.core - INFO - Starting established connection
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.26.66.66:36666
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:   tcp://172.26.66.66:37206
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -       Start worker at:   tcp://172.26.66.66:34548
distributed.worker - INFO -          Listening to:   tcp://172.26.66.66:34548
distributed.worker - INFO -          dashboard at:         172.26.66.66:35572
distributed.worker - INFO - Waiting to connect to:   tcp://172.26.66.66:37206
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                   3.91 GiB
distributed.worker - INFO -       Local Directory: /home/users/hkhatri/Git_Repo/AMOC_Variability_DePreSys_MET/Python_Scripts/dask-worker-space/worker-x24rm60e
distributed.worker - INFO - -------------------------------------------------
distributed.diskutils - INFO - Found stale lock file and directory '/home/users/hkhatri/Git_Repo/AMOC_Variability_DePreSys_MET/Python_Scripts/dask-worker-space/worker-ot0kyrxk', purging
distributed.scheduler - INFO - Register worker <WorkerState 'tcp://172.26.66.66:34363', name: 2, status: undefined, memory: 0, processing: 0>
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.26.66.66:34363
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:   tcp://172.26.66.66:37206
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Start worker at:   tcp://172.26.66.66:38301
distributed.worker - INFO -          Listening to:   tcp://172.26.66.66:38301
distributed.worker - INFO -          dashboard at:         172.26.66.66:42773
distributed.core - INFO - Starting established connection
distributed.worker - INFO - Waiting to connect to:   tcp://172.26.66.66:37206
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                   3.91 GiB
distributed.worker - INFO -       Local Directory: /home/users/hkhatri/Git_Repo/AMOC_Variability_DePreSys_MET/Python_Scripts/dask-worker-space/worker-dv7ovuox
distributed.worker - INFO - -------------------------------------------------
distributed.scheduler - INFO - Register worker <WorkerState 'tcp://172.26.66.66:34548', name: 6, status: undefined, memory: 0, processing: 0>
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.26.66.66:34548
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:   tcp://172.26.66.66:37206
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.diskutils - INFO - Found stale lock file and directory '/home/users/hkhatri/Git_Repo/AMOC_Variability_DePreSys_MET/Python_Scripts/dask-worker-space/worker-zcdxzuaw', purging
distributed.scheduler - INFO - Register worker <WorkerState 'tcp://172.26.66.66:38301', name: 4, status: undefined, memory: 0, processing: 0>
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.26.66.66:38301
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:   tcp://172.26.66.66:37206
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -       Start worker at:   tcp://172.26.66.66:36121
distributed.worker - INFO -          Listening to:   tcp://172.26.66.66:36121
distributed.worker - INFO -          dashboard at:         172.26.66.66:35474
distributed.worker - INFO - Waiting to connect to:   tcp://172.26.66.66:37206
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                   3.91 GiB
distributed.worker - INFO -       Local Directory: /home/users/hkhatri/Git_Repo/AMOC_Variability_DePreSys_MET/Python_Scripts/dask-worker-space/worker-l3r1esxv
distributed.worker - INFO - -------------------------------------------------
distributed.scheduler - INFO - Register worker <WorkerState 'tcp://172.26.66.66:36121', name: 7, status: undefined, memory: 0, processing: 0>
distributed.scheduler - INFO - Starting worker compute stream, tcp://172.26.66.66:36121
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:   tcp://172.26.66.66:37206
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
Traceback (most recent call last):
  File "/home/users/hkhatri/Git_Repo/AMOC_Variability_DePreSys_MET/Python_Scripts/Drift_ocean_3D_save_monthwise.py", line 90, in <module>
    ds_var = processDataset(ds, year1, year2, lead_year)
  File "/home/users/hkhatri/Git_Repo/AMOC_Variability_DePreSys_MET/Python_Scripts/Drift_ocean_3D_save_monthwise.py", line 33, in processDataset
    ds2 = ds2.map_blocks(numpy.copy)
NameError: name 'numpy' is not defined
distributed.scheduler - INFO - Receive client connection: Client-134a1543-9493-11ec-a6ce-f452141d4040
distributed.core - INFO - Starting established connection
distributed.worker - INFO - Run out-of-band function 'stop'
distributed.scheduler - INFO - Scheduler closing...
distributed.scheduler - INFO - Scheduler closing all comms
distributed.worker - INFO - Stopping worker at tcp://172.26.66.66:34363
distributed.worker - INFO - Stopping worker at tcp://172.26.66.66:36666
distributed.worker - INFO - Stopping worker at tcp://172.26.66.66:45002
distributed.worker - INFO - Stopping worker at tcp://172.26.66.66:34548
distributed.worker - INFO - Stopping worker at tcp://172.26.66.66:38301
distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://172.26.66.66:45002', name: 5, status: running, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.26.66.66:45002
distributed.worker - INFO - Stopping worker at tcp://172.26.66.66:36121
distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://172.26.66.66:36666', name: 3, status: running, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.26.66.66:36666
distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://172.26.66.66:34363', name: 2, status: running, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.26.66.66:34363
distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://172.26.66.66:34548', name: 6, status: running, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.26.66.66:34548
distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://172.26.66.66:38301', name: 4, status: running, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.26.66.66:38301
distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://172.26.66.66:36121', name: 7, status: running, memory: 0, processing: 0>
distributed.core - INFO - Removing comms to tcp://172.26.66.66:36121
distributed.scheduler - INFO - Lost all workers
tornado.application - ERROR - Exception in callback functools.partial(<function TCPServer._handle_connection.<locals>.<lambda> at 0x7f3a8ea29a60>, <Task finished name='Task-15123' coro=<BaseTCPListener._handle_stream() done, defined at /home/users/hkhatri/miniconda3/lib/python3.9/site-packages/distributed/comm/tcp.py:530> exception=ValueError('invalid operation on non-started TCPListener')>)
Traceback (most recent call last):
  File "/home/users/hkhatri/miniconda3/lib/python3.9/site-packages/tornado/ioloop.py", line 741, in _run_callback
    ret = callback()
  File "/home/users/hkhatri/miniconda3/lib/python3.9/site-packages/tornado/tcpserver.py", line 331, in <lambda>
    gen.convert_yielded(future), lambda f: f.result()
  File "/home/users/hkhatri/miniconda3/lib/python3.9/site-packages/distributed/comm/tcp.py", line 536, in _handle_stream
    logger.debug("Incoming connection from %r to %r", address, self.contact_address)
  File "/home/users/hkhatri/miniconda3/lib/python3.9/site-packages/distributed/comm/tcp.py", line 572, in contact_address
    host, port = self.get_host_port()
  File "/home/users/hkhatri/miniconda3/lib/python3.9/site-packages/distributed/comm/tcp.py", line 553, in get_host_port
    self._check_started()
  File "/home/users/hkhatri/miniconda3/lib/python3.9/site-packages/distributed/comm/tcp.py", line 528, in _check_started
    raise ValueError("invalid operation on non-started TCPListener")
ValueError: invalid operation on non-started TCPListener
distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://172.26.66.66:60468 remote=tcp://172.26.66.66:37206>
Traceback (most recent call last):
  File "/home/users/hkhatri/miniconda3/lib/python3.9/site-packages/distributed/batched.py", line 94, in _background_send
    nbytes = yield self.comm.write(
  File "/home/users/hkhatri/miniconda3/lib/python3.9/site-packages/tornado/gen.py", line 762, in run
    value = future.result()
  File "/home/users/hkhatri/miniconda3/lib/python3.9/site-packages/distributed/comm/tcp.py", line 248, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://172.26.66.66:60464 remote=tcp://172.26.66.66:37206>
Traceback (most recent call last):
  File "/home/users/hkhatri/miniconda3/lib/python3.9/site-packages/distributed/batched.py", line 94, in _background_send
    nbytes = yield self.comm.write(
  File "/home/users/hkhatri/miniconda3/lib/python3.9/site-packages/tornado/gen.py", line 762, in run
    value = future.result()
  File "/home/users/hkhatri/miniconda3/lib/python3.9/site-packages/distributed/comm/tcp.py", line 248, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://172.26.66.66:60470 remote=tcp://172.26.66.66:37206>
Traceback (most recent call last):
  File "/home/users/hkhatri/miniconda3/lib/python3.9/site-packages/distributed/batched.py", line 94, in _background_send
    nbytes = yield self.comm.write(
  File "/home/users/hkhatri/miniconda3/lib/python3.9/site-packages/tornado/gen.py", line 762, in run
    value = future.result()
  File "/home/users/hkhatri/miniconda3/lib/python3.9/site-packages/distributed/comm/tcp.py", line 248, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://172.26.66.66:60466 remote=tcp://172.26.66.66:37206>
Traceback (most recent call last):
  File "/home/users/hkhatri/miniconda3/lib/python3.9/site-packages/distributed/batched.py", line 94, in _background_send
    nbytes = yield self.comm.write(
  File "/home/users/hkhatri/miniconda3/lib/python3.9/site-packages/tornado/gen.py", line 762, in run
    value = future.result()
  File "/home/users/hkhatri/miniconda3/lib/python3.9/site-packages/distributed/comm/tcp.py", line 248, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://172.26.66.66:60472 remote=tcp://172.26.66.66:37206>
Traceback (most recent call last):
  File "/home/users/hkhatri/miniconda3/lib/python3.9/site-packages/distributed/batched.py", line 94, in _background_send
    nbytes = yield self.comm.write(
  File "/home/users/hkhatri/miniconda3/lib/python3.9/site-packages/tornado/gen.py", line 762, in run
    value = future.result()
  File "/home/users/hkhatri/miniconda3/lib/python3.9/site-packages/distributed/comm/tcp.py", line 248, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://172.26.66.66:60474 remote=tcp://172.26.66.66:37206>
Traceback (most recent call last):
  File "/home/users/hkhatri/miniconda3/lib/python3.9/site-packages/distributed/batched.py", line 94, in _background_send
    nbytes = yield self.comm.write(
  File "/home/users/hkhatri/miniconda3/lib/python3.9/site-packages/tornado/gen.py", line 762, in run
    value = future.result()
  File "/home/users/hkhatri/miniconda3/lib/python3.9/site-packages/distributed/comm/tcp.py", line 248, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
